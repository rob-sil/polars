searchState.loadedDescShard("polars_io", 0, "Options for Hive partitioning.\nInterface with cloud storage through the object_store …\n(De)serializing CSV files\nTake the SerReader and return a parsed DataFrame.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if the path is a cloud url.\n(De)serialize JSON files.\nCreate a new instance of the <code>[SerReader]</code>\nReading Apache parquet files.\nMake sure that all columns are contiguous in memory by …\nValid compressions\nRead Apache Avro format into a <code>DataFrame</code>\nWrite a <code>DataFrame</code> to Apache Avro format\nValid compressions\nDeflate\nDeflate\nSnappy\nSnappy\nGet arrow schema of the avro File, this is faster than a …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nGet schema of the Avro File\nColumns to select/ project\nSet the compression used. Defaults to None.\nStop reading when <code>n</code> rows are read.\nSet the reader’s column projection. This counts from 0, …\nA location on cloud storage, may have wildcards.\nAdaptor which wraps the asynchronous interface of …\nPolars specific wrapper for <code>Arc&lt;dyn ObjectStore&gt;</code> that …\nThe bucket name.\nBuild an <code>ObjectStore</code> based on the URL and passed in url. …\nExecutes the given command directly.\nThe path components that need to be expanded.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nList files with a prefix derived from the pattern.\nFetch the metadata of the parquet file, do not memoize it.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nConstructs a new CloudWriter from a path and an optional …\nParse a CloudLocation from an url.\nConstruct a new CloudWriter, re-using the given …\nThe prefix inside the bucket, this will be the full key …\nQueues the given command for further execution.\nThe scheme (s3, …).\nPerforms a set of actions within a synchronous update.\nMaster key for accessing storage account\nAWS Access Key\nThe name of the azure storage account\nConfiguration keys for <code>AmazonS3Builder</code>\nApplication credentials path\nTenant id used in oauth flows\nConfiguration keys for <code>MicrosoftAzureBuilder</code>\nBucket name\nBucket name\nSet the checksum algorithm for this client\nClient options\nClient options\nClient options\nService principal client id for authorizing requests\nService principal client secret for authorizing requests\nOptions to connect to various cloud providers.\nConfigure how to provide conditional put operations\nSet the container credentials relative URI\nContainer name\nConfigure how to provide <code>copy_if_not_exists</code>\nDefault region\nDisable tagging objects\nDisables tagging objects\nEncryption options\nSets custom endpoint for communicating with AWS S3.\nOverride the endpoint used to communicate with blob storage\nFile containing token for Azure AD workload identity …\nConfiguration keys for <code>GoogleCloudStorageBuilder</code>\nFall back to ImdsV1\nSet the instance metadata endpoint\nEndpoint to request a imds managed identity token\nMsi resource id for use with managed identity …\nObject id for use with managed identity authentication\nRegion\nEnable Support for S3 Express One Zone\nShared access signature.\nSecret Access Key\nPath to the service account file\nThe serialized service account key.\nSkip signing request\nSkip signing requests\nToken to use for requests (passed to underlying provider)\nBearer token\nAvoid computing payload checksum when calculating …\nUse azure cli for acquiring access token\nUse object store with azurite storage emulator\nUse object store with url scheme …\nIf virtual hosted style request has to be used\nBuild the <code>object_store::ObjectStore</code> implementation for AWS.\nBuild the <code>object_store::ObjectStore</code> implementation for …\nBuild the <code>object_store::ObjectStore</code> implementation for GCP.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nParse a configuration from a Hashmap. This is the …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nSet the configuration for AWS connections. This is the …\nSet the configuration for Azure connections. This is the …\nSet the configuration for GCP connections. This is the …\nMultiple values that are used for all columns\nA single value that’s used for all columns\nThis puts quotes around every field. Always.\nCreate a new DataFrame by reading a csv file.\nWrite a DataFrame to csv.\nUtf8 encoding and unknown bytes are replaced with �\nA string that indicates the start of a comment line. This …\nTuples that map column names to null value of that column\nThis puts quotes around fields only when necessary.\nNever quote any fields, even if it would produce invalid …\nThis puts quotes around all fields that are non-numeric. …\nOptions to serialize logical types to CSV.\nA single byte character that indicates the start of a …\nUtf8 encoding\nSets the comment prefix from <code>CsvParserOptions</code> for internal …\nRead the number of rows without parsing columns useful for …\nUsed for <code>DataType::Date</code>.\nUsed for <code>DataType::Datetime</code>.\nRead the file and create the DataFrame.\nWrites the header of the csv file if not done already. …\nUsed for <code>DataType::Float64</code> and <code>DataType::Float32</code>.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nThis is the recommended way to create a csv reader as this …\nSet whether the CSV file has headers\nSet whether to write UTF-8 BOM.\nSet whether to write headers.\nSet the CSV reader to infer the schema of the file\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nString appended after every row.\nReduce memory consumption at the expense of performance\nCreate a new CsvReader from a file/ stream\nCreates a new <code>CommentPrefix</code>. If <code>Multi</code> variant is used and …\nCreates a new <code>CommentPrefix</code> for the <code>Single</code> variant.\nNull value representation.\nQuoting character.\nRaise an error if CSV is empty (otherwise return an empty …\nSets the size of the sample taken from the CSV file. The …\nUsed as separator.\nUsed for <code>DataType::Time</code>.\nTruncate lines that are longer than the schema.\nSet the batch size to use while writing the CSV.\nSets the chunk size used by the parser. This influences …\nColumns to select/ project\nSet the comment prefix for this instance. Lines starting …\nSet the CSV file’s date format.\nSet the CSV file’s datetime format.\nParse floats with decimals.\nOverwrite the schema with the dtypes in this given Schema. …\nOverwrite the dtypes in the schema in the order of the …\nSet  <code>CsvEncoding</code>\nSet the CSV file’s float precision.\nContinue with next batch when a ParserError is encountered.\nSet the CSV file’s line terminator.\nTreat missing fields as null.\nTry to stop parsing when <code>n</code> rows are parsed. During …\nSet the number of threads used in CSV reading. The default …\nSet the CSV file’s null value representation.\nSet values that will be interpreted as missing/ null. Note …\nThe preferred way to initialize this builder. This allows …\nSet the reader’s column projection. This counts from 0, …\nSet the <code>char</code> used as quote char. The default is <code>b&#39;&quot;&#39;</code>. If …\nSet the single byte character used for quoting.\nSet the CSV file’s quoting behavior. See more on …\nRechunk the DataFrame to contiguous memory after the CSV …\nAdd a row index column.\nSet the CSV file’s schema. This only accepts datatypes …\nSet the CSV file’s column separator as a byte character\nSet the CSV file’s column separator as a byte character.\nSkip the first <code>n</code> rows during parsing. The header will be …\nSkip these rows after the header\nSet the CSV file’s time format.\nAutomatically try to parse dates/ datetimes and time. If …\nWrite a batch to the csv writer.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nInfer the schema of a CSV file by reading through the …\ncheck if csv file is compressed\nArrow-deserialized parquet Statistics of a file\nArrow-deserialized parquet Statistics of a file\nDeserializes the statistics in the column chunks from a …\nnumber of dictinct values. This is a <code>UInt64Array</code> for …\nnumber of dictinct values. This is a <code>UInt64Array</code> for …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nMaximum\nMaximum\nMinimum\nMinimum\nnumber of nulls. This is a <code>UInt64Array</code> for non-nested types\nnumber of nulls. This is a <code>UInt64Array</code> for non-nested types\nCompression codec\nRead Arrows IPC format into a DataFrame\nAn Arrow IPC reader implemented on top of …\nRead Arrows Stream IPC format into a DataFrame\nWrite a DataFrame to Arrow’s Streaming IPC format\nWrite a DataFrame to Arrow’s IPC format\nLZ4 (framed)\nZSTD\nGet arrow schema of the Ipc Stream File, this is faster …\nData page compression\nWrites the footer of the IPC file.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nmaintain the order the data was processed\nSet if the file is to be memory_mapped. Only works with …\nGet arrow schema of the Ipc File.\nGet schema of the Ipc Stream File\nColumns to select/ project\nColumns to select/ project\nSet the compression used. Defaults to None.\nSet the compression used. Defaults to None.\nSet the compression used. Defaults to None.\nSet the compression used. Defaults to None.\nSet the extension. Defaults to “.ipc”.\nSet the extension. Defaults to “.ipc”.\nStop reading when <code>n</code> rows are read.\nStop reading when <code>n</code> rows are read.\nSet the reader’s column projection. This counts from 0, …\nSet the reader’s column projection. This counts from 0, …\nAdd a row index column.\nAdd a row index column.\nWrite a batch to the parquet writer.\nA single JSON array containing each DataFrame row as an …\nThe format to use to write the DataFrame to JSON: <code>Json</code> (a …\nEach DataFrame row is serialized as a JSON object on a …\nReads JSON in one of the formats in <code>JsonFormat</code> into a …\nWrites a DataFrame to JSON.\nTake the SerReader and return a parsed DataFrame.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nSet the JSON reader to infer the schema of the file. …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nmaintain the order the data was processed\nCreate a new <code>JsonWriter</code> writing to <code>buffer</code> with format …\nSet the batch size (number of records to load at one time)\nReturn a <code>null</code> if an error occurs during parsing.\nSet the reader’s column projection: the names of the …\nSet the JSON file’s schema\nOverwrite parts of the inferred schema.\nWrite a batch to the json writer.\nTrait used to get a hold to file handler or to the …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nA <code>StructArray</code> is a nested <code>Array</code> with an optional validity …\nArcs this array into a <code>std::sync::Arc&lt;dyn Array&gt;</code>.\nBoxes this array into a <code>Box&lt;dyn Array&gt;</code>.\nReturns the fields of this <code>StructArray</code>.\nReturns the argument unchanged.\nReturns the argument unchanged.\nThis is the recommended way to create a json reader as …\nReturns the fields the <code>DataType::Struct</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nDeconstructs the <code>StructArray</code> into its individual …\nReturns an iterator of <code>Option&lt;Box&lt;dyn Array&gt;&gt;</code>\nReduce memory consumption at the expense of performance\nCreate a new JsonLineReader from a file/ stream\nReturns a new <code>StructArray</code>\nCreates an empty <code>StructArray</code>.\nCreates a null <code>StructArray</code> of length <code>length</code>.\nSets the validity of this array.\nSlices this <code>StructArray</code>.\nSlices this <code>StructArray</code>.\nReturns this array sliced.\nReturns this array sliced.\nTakes the validity of this array, leaving it without a …\nReturns a new <code>StructArray</code>.\nThe optional validity.\nReturns the values of this <code>StructArray</code>.\nReturns an iterator of <code>Box&lt;dyn Array&gt;</code>\nSets the chunk size used by the parser. This influences …\nSet values as <code>Null</code> if parsing fails because of schema …\nReturns this array with a new validity.\nAutomatically determine over which unit to parallelize …\nParallelize over the columns\nMetadata for a Parquet file.\nDon’t parallelize\nA Parquet reader on top of the async object_store API. …\nRead Apache parquet format into a DataFrame.\nWrite a DataFrame to parquet format\nParallelize over the row groups\nRepresents a valid zstd compression level.\nRepresents a valid zstd compression level.\nReturns column order for <code>i</code>th column in this file. If …\nColumn (sort) order used for <code>min</code> and <code>max</code> values of each …\nData page compression\nData page compression\nString message for application that wrote this file.\nif <code>None</code> will be 1024^2 bytes\nif <code>None</code> will be 1024^2 bytes\nWrite the given DataFrame in the writer <code>W</code>. Returns the …\nWrites the footer of the parquet file. Returns the total …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nSerializes itself to thrift’s …\nTurn the batched reader into an iterator.\nreturns the metadata\nkey_value_metadata of this file.\nmaintain the order the data was processed\nmaintain the order the data was processed\nCreate a new <code>ParquetReader</code> from an existing <code>Reader</code>.\nCreate a new writer\nNumber of rows in the parquet file.\nnumber of rows in the file.\nRead the parquet file in parallel (default). The single …\nIf <code>None</code> will be all written to a single row group.\nIf <code>None</code> will be all written to a single row group.\nThe row groups of this file\n<code>Schema</code> of the file.\nReturns the <code>SchemaDescriptor</code> that describes schema of this …\nschema descriptor.\nTry to reduce memory pressure at the expense of …\nSerialize columns in parallel\nCompute and write column statistics.\nCompute and write column statistics.\nDeserializes <code>crate::parquet::thrift_format::FileMetaData</code> …\nUse statistics in the parquet to determine if pages can be …\nUse statistics in the parquet to determine if pages can be …\nversion of this file.\nColumns to select/ project\nSet the compression used. Defaults to <code>Zstd</code>.\nSets the maximum bytes size of a data page. If <code>None</code> will …\nStop parsing when <code>n</code> rows are parsed. By settings this …\nSet the reader’s column projection. This counts from 0, …\nSet the row group size (in number of rows) during writing. …\nAdd a row index column.\nSet the <code>Schema</code> if already known. This must be exactly the …\nCompute and write statistic\nWrite a batch to the parquet writer.\nWrite a DataFrame with disk partitioning\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nWrite the parquet file in parallel (default).\nKeep track of rayon threads that drive the runtime. Every …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nSpawns a future onto the Tokio runtime (see …\nA collection of column stats with a known schema.\nStatistics of the values in a column.\nCan take &amp;dyn Statistics and determine of a file should be …\nReturns the <code>ColumnStats</code> of all columns in the batch, if …\nReturns the <code>DataType</code> of the column.\nTake a <code>DataFrame</code> and produces a boolean <code>Series</code> that serves …\nReturns the argument unchanged.\nReturns the argument unchanged.\nConstructs a new <code>ColumnStats</code> from a single-value Series.\nConstructs a new <code>ColumnStats</code> with only the <code>Field</code> …\nReturns the maximum value of each row group of the column.\nReturns the minimum value of each row group of the column.\nReturns the null count of each row group of the column.\nReturns the <code>ColumnStats</code> of a single column in the batch.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nConstructs a new <code>ColumnStats</code>.\nConstructs a new <code>BatchStats</code>.\nReturns the null count of the column.\nReturns the number of rows in the batch.\nReturns the <code>Schema</code> of the batch.\nReturns the maximum value of the column as a single-value …\nReturns the minimum value of the column as a single-value …\nReturns the minimum and maximum values of the column as a …\nChecks if the projected columns are equal\nChecks if the projected columns are equal\nCompute <code>remaining_rows_to_read</code> to be taken per file up …")